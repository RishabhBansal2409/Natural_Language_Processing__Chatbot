{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building classifier models for Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation libraries\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Text Processing libraries\n",
    "import nltk\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dialogues dataset\n",
    "dialogues = pd.read_csv(\"dialogues.tsv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the posts dataset\n",
    "posts = pd.read_csv(\"tagged_posts.tsv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Okay -- you're gonna need to learn how to lie.</td>\n",
       "      <td>dialogue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm kidding.  You know how sometimes you just ...</td>\n",
       "      <td>dialogue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Like my fear of wearing pastels?</td>\n",
       "      <td>dialogue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I figured you'd get to the good stuff eventually.</td>\n",
       "      <td>dialogue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank God!  If I had to hear one more story ab...</td>\n",
       "      <td>dialogue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       tag\n",
       "0     Okay -- you're gonna need to learn how to lie.  dialogue\n",
       "1  I'm kidding.  You know how sometimes you just ...  dialogue\n",
       "2                   Like my fear of wearing pastels?  dialogue\n",
       "3  I figured you'd get to the good stuff eventually.  dialogue\n",
       "4  Thank God!  If I had to hear one more story ab...  dialogue"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting the dialogues dataset\n",
    "dialogues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>Calculate age in C#</td>\n",
       "      <td>c#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>Filling a DataSet or DataTable from a LINQ que...</td>\n",
       "      <td>c#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>Reliable timer in a console application</td>\n",
       "      <td>c#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>Best way to allow plugins for a PHP application</td>\n",
       "      <td>php</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59</td>\n",
       "      <td>How do I get a distinct, ordered list of names...</td>\n",
       "      <td>c#</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                              title  tag\n",
       "0        9                                Calculate age in C#   c#\n",
       "1       16  Filling a DataSet or DataTable from a LINQ que...   c#\n",
       "2       39            Reliable timer in a console application   c#\n",
       "3       42    Best way to allow plugins for a PHP application  php\n",
       "4       59  How do I get a distinct, ordered list of names...   c#"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting the posts dataset\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Posts: 2171575\n",
      "Number of Dialogues: 218609\n"
     ]
    }
   ],
   "source": [
    "# Checking the no of entries in each of the dataset\n",
    "print(\"Number of Posts:\",len(posts))\n",
    "print(\"Number of Dialogues:\",len(dialogues))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating training data for intent classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the first 0.2 million rows from either of the dataset\n",
    "texts  =  list(dialogues[:200000].text.values) + list(posts[:200000].title.values)\n",
    "labels =  ['dialogue']*200000 + ['stackoverflow']*200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with texts and labels as columns\n",
    "data = pd.DataFrame({'text':texts,'target':labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a function which performs basic text preprocessing tasks\n",
    "\n",
    "**The tasks are as follows :-**\n",
    "\n",
    "a) Replace the characterts such as [/(){}\\[\\]\\|@,;] with space.  \n",
    "b) Keep only the alphanumeric ones (and some special ones).  \n",
    "c) Removing the english stopwords as these do not serve any purpose.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prepare(text):\n",
    "    \"\"\"Performs tokenization and simple preprocessing.\"\"\"\n",
    "    \n",
    "    replace_by_space_re = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    bad_symbols_re = re.compile('[^0-9a-z #+_]')\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "    text = text.lower()\n",
    "    text = replace_by_space_re.sub(' ', text)\n",
    "    text = bad_symbols_re.sub('', text)\n",
    "    text = ' '.join([x for x in text.split() if x and x not in stopwords_set])\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken (in sec) to process the above operation : 298.77839374542236\n"
     ]
    }
   ],
   "source": [
    "# Performing data cleaning task on each of the text present in the data \n",
    "current_time = time.time()\n",
    "data['text'] = data['text'].apply(lambda x : text_prepare(x))\n",
    "print('Time Taken (in sec) to process the above operation : {}'.format(time.time() - current_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size = 360000, test size = 40000\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into training and testing for model building and prediction validation purposes\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'],data['target'],test_size = .1 , random_state=0)\n",
    "\n",
    "# Printing the sizes of training and testing dataset\n",
    "print('Train size = {}, test size = {}'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating an Intent classifier \n",
    "\n",
    "### Intent classifier classifies the question in two categories namely :- Dialogue or Stack Overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a new directory named as resources to keep our models and vectorizers\n",
    "!mkdir resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a function which returns the TF-IDF matrix for each of the training and testing datasets.\n",
    "\n",
    "TF-IDF Matrix stands for Term Frequency and the Inverse Document frequency matrix . TF is described as the no of occurences of a word in a document  whereas IDF is the inverse of the document frequency where document frequency is the no of times the word has occured across all the documents divided by the no of documents.TF-IDF is the product of TF and IDF reflecting each term's presumed importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_features(X_train, X_test, vectorizer_path):\n",
    "    \"\"\"Performs TF-IDF transformation and dumps the model.\"\"\"\n",
    "    tfv = TfidfVectorizer(dtype=np.float32, min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "    \n",
    "    X_train = tfv.fit_transform(X_train)\n",
    "    X_test = tfv.transform(X_test)\n",
    "    \n",
    "    pickle.dump(tfv,vectorizer_path)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken (in sec) to process the above operation : 37.037238359451294\n"
     ]
    }
   ],
   "source": [
    "# Splitting the TF-IDF matrix into training and testing for model building and prediction validation purposes\n",
    "current_time = time.time()\n",
    "X_train_tfidf, X_test_tfidf = tfidf_features(X_train, X_test, open(\"resources/tfidf.pkl\",'wb'))\n",
    "print('Time Taken (in sec) to process the above operation : {}'.format(time.time() - current_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a Logistic Regression model to classify the intent of the question (Dialogue or StackOverflow)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "intent_recognizer = LogisticRegression(C=10,random_state=0)\n",
    "intent_recognizer.fit(X_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 0.989625\n"
     ]
    }
   ],
   "source": [
    "# Checking test accuracy\n",
    "y_test_pred = intent_recognizer.predict(X_test_tfidf)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('Test accuracy = {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['stackoverflow', 'dialogue', 'stackoverflow', ..., 'dialogue',\n",
       "       'stackoverflow', 'stackoverflow'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping the intent recognizer model in the resources directory for future use\n",
    "pickle.dump(intent_recognizer, open(\"resources/intent_clf.pkl\" , 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating a Programming Language Classifier\n",
    "\n",
    "### Classifier classifies the question into one of the below programming language based on the question's text.\n",
    "\n",
    "**c# ,php , c_cpp , python , ruby , java , javascript , vb , r , swift**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['c#', 'php', 'c_cpp', 'python', 'ruby', 'java', 'javascript', 'vb',\n",
       "       'r', 'swift'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identifying unique programming languages\n",
    "posts['tag'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing data cleaning on each of the title present in the data\n",
    "X = posts['title'].apply(lambda x : text_prepare(x))\n",
    "y = posts['tag'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size = 1737260, test size = 434315\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into training and testing for model building and prediction validation purposes\n",
    "\n",
    "X_train_plc, X_test_plc, y_train_plc, y_test_plc = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Printing the sizes of training and testing dataset\n",
    "print('Train size = {}, test size = {}'.format(len(X_train_plc), len(X_test_plc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken (in sec) to process the above operation : 98.4446370601654\n"
     ]
    }
   ],
   "source": [
    "# Loading the stored TF_IDF_vectorizer object to build a new TF-IDF matrix\n",
    "current_time = time.time()\n",
    "vectorizer = pickle.load(open(\"resources/tfidf.pkl\", 'rb'))\n",
    "X_train_plc_tfidf, X_test_plc_tfidf = vectorizer.transform(X_train_plc), vectorizer.transform(X_test_plc)\n",
    "print('Time Taken (in sec) to process the above operation : {}'.format(time.time() - current_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken (in sec) to process the above operation : 566.328616142273\n"
     ]
    }
   ],
   "source": [
    "# Building a Logistic Regression model to classify the question into one of the programming language(for StackOverflow only)\n",
    "current_time = time.time()\n",
    "tag_classifier = OneVsRestClassifier(LogisticRegression(C=5 , random_state=0 , max_iter = 100))\n",
    "tag_classifier.fit(X_train_plc_tfidf,y_train_plc)\n",
    "print('Time Taken (in sec) to process the above operation : {}'.format(time.time() - current_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 0.8043079331821373\n"
     ]
    }
   ],
   "source": [
    "# Checking test accuracy\n",
    "y_test_plc_pred = tag_classifier.predict(X_test_plc_tfidf)\n",
    "test_accuracy = accuracy_score(y_test_plc, y_test_plc_pred)\n",
    "print('Test accuracy = {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping the tag classifier model in the resources directory for future use\n",
    "pickle.dump(tag_classifier, open(\"resources/tag_clf.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Building and Storing title database embeddings/word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are intending to build word vectors corresponding to each of the titles using the [pre-trained word vectors](https://code.google.com/archive/p/word2vec/) from Google.  \n",
    "\n",
    "Word Vectors are basically representation of words in a high dimensional space here the no of dimensions being 300 for a word.  \n",
    "\n",
    "Word Vectors are useful since it enables us to know how similar two sentences are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263.8938066959381\n"
     ]
    }
   ],
   "source": [
    "# Loading the google's pre trained Word2 Vec model.\n",
    "current_time = time.time()\n",
    "import gensim.downloader as api\n",
    "path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True) \n",
    "print(time.time() - current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a function which will convert the input text to an embedding. \n",
    "\n",
    "These embeddings will be used to give an answer to user's stack overflow question by getting the most similar question using the cosine similarity metric ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec(question, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        embeddings: dict where the key is a word and a value is its' embedding\n",
    "        dim: size of the representation\n",
    "\n",
    "        result: vector representation for the question\n",
    "    \"\"\"\n",
    "    word_tokens = question.split(\" \")\n",
    "    question_len = len(word_tokens)\n",
    "    \n",
    "    # Initializing an array with the rows as no of words in the text and 300 as the columns\n",
    "    question_mat = np.zeros((question_len,dim), dtype = np.float32)\n",
    "    \n",
    "    # Checking if the word exists in the google pre-trained model \n",
    "    for idx, word in enumerate(word_tokens):\n",
    "        if word in embeddings:\n",
    "            # If word is there then populate the array with the embeddings of the word \n",
    "            question_mat[idx,:] = embeddings[word]\n",
    "            \n",
    "    # remove zero-rows which stand for OOV words       \n",
    "    question_mat = question_mat[~np.all(question_mat == 0, axis = 1)]\n",
    "    \n",
    "    # Compute the mean of each word along the sentence\n",
    "    if question_mat.shape[0] > 0:\n",
    "        vec = np.array(np.mean(question_mat, axis = 0), dtype = np.float32).reshape((1,dim))\n",
    "    else:\n",
    "        vec = np.zeros((1,dim), dtype = np.float32)\n",
    "        \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a new directory named as resources\n",
    "!mkdir embeddings_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken (in sec) to process the above operation : 578.8078963756561\n"
     ]
    }
   ],
   "source": [
    "# Populating an array with the title's embeddings \n",
    "current_time = time.time()\n",
    "\n",
    "counts_by_tag = posts.groupby(by=['tag'])[\"tag\"].count().reset_index(name = 'count').sort_values(['count'], ascending = False)\n",
    "\n",
    "# Creating a list of tupes where first element of tuple is the tag an second is the count\n",
    "counts_by_tag = list(zip(counts_by_tag['tag'],counts_by_tag['count']))\n",
    "\n",
    "# Iterating over each tag\n",
    "for tag, count in counts_by_tag:\n",
    "    tag_posts = posts[posts['tag'] == tag]\n",
    "    tag_post_ids = tag_posts['post_id'].values\n",
    "    \n",
    "    # Intializng an array with the rows as titles belonging to a tag and columns as 300 \n",
    "    tag_vectors = np.zeros((count, 300), dtype=np.float32)\n",
    "    \n",
    "    # Ierating over each title belonging to a tag\n",
    "    for i, title in enumerate(tag_posts['title']):\n",
    "        tag_vectors[i, :] = question_to_vec(title, model, 300)\n",
    "        \n",
    "    # Dump post ids and vectors to a file.\n",
    "    filename = 'resources/embeddings_folder/'+ tag + '.pkl'\n",
    "    pickle.dump((tag_post_ids, tag_vectors), open(filename, 'wb'))\n",
    "print('Time Taken (in sec) to process the above operation : {}'.format(time.time() - current_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a function which when given a question and tag will retrieve the most similar question post_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken (in sec) to process the above operation : 2.009498357772827\n",
      "We can find the answer to the question at : https://stackoverflow.com/questions/5947137\n"
     ]
    }
   ],
   "source": [
    "def get_similar_question(question,tag):\n",
    "    current_time = time.time()\n",
    "    # Get the path where all the question embeddings are kept and also loading the post_ids and title_embeddings\n",
    "    embeddings_path = 'resources/embeddings_folder/' + tag + \".pkl\"\n",
    "    post_ids, post_embeddings = pickle.load(open(embeddings_path, 'rb'))\n",
    "    # Get the embeddings for the question\n",
    "    question_vec = question_to_vec(question, model, 300)\n",
    "    # Find index of most similar post\n",
    "    best_post_index = pairwise_distances_argmin(question_vec,post_embeddings)\n",
    "    print('Time Taken (in sec) to process the above operation : {}'.format(time.time() - current_time))\n",
    "    # Return best post id\n",
    "    return post_ids[best_post_index]\n",
    "\n",
    "answer_post_id =get_similar_question(\"how to use list comprehension in python?\",'python')[0]\n",
    "\n",
    "print(\"We can find the answer to the question at : https://stackoverflow.com/questions/{}\".format(answer_post_id))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
